It looks long, but is actually easy. The servers S1 and S2  are just minor changes from the time server you did in Assgn1, and the client is the same as the time client. Only L needs some work. For L, your logic is simple:

1. Open a socket to wait for client connection.
2. Do a poll() call to wait on the socket or a timeout for 5 sec
3. On timeout, open connection to S1 to get its load, then open connection to S2 and get its load, then come back and wait on poll()
4. On client connection, see which of S1 and S2 you need to go to , open connection to that and get time. Send the time back through the newsockfd for the client. Original sockfd comes back to wait on poll() for next client.

Just a flavor of a load balancer in practice, which is an omnipresent system in the backend. In practice, the chosen server can connect to the client directly also to not have the LB in the middle.

Some of you have pointed out a problem in Assgn 3 and some of you are trying complex solutions using fork/thread etc. This is supposed to be 1  week individual assignment so I do not want you to do anything complex. Using fork/thread will bring in additional issues as there will be a shared table. so do NOT use fork etc., see below.

The problem is this. Suppose you are waiting in poll() for 5 seconds or a client connection. Now after 3 seconds a client connection comes and is handled as per the details. Now when you go back to poll(), how long should you wait for? Ideally should be 2 seconds only. But then poll() does not give you any indication as to how much of the timeout has elapsed when you got out (so the 3 second is not known).

One simple possibility is to call with 5 seconds again But then, if one client request comes every 5 seconds, the loads will never get updated.

So do this simple thing. Keep a variable T to keep time. When you call poll() with full 5 second timeout, just before calling, get the system time and store it there (upto granularity of second).  Now if you come out of poll() due to timeout, get the system time again in T (reset T to start of 5 sec interval), and then after getting the loads from servers, call poll() with 5 second timeout. If you come out due to client request, after servicing the client, get the system time again in a temp variable, take the difference with T to see how much time has elapsed, and call poll() with the remainder time.

This is approximate, but given that 5 second is large, and time to service requests is small in this case, the approximation is ok.

In the last mail, when I said no fork()/thread, I may have created another confusion. The Load Balancer is a concurrent TCP server, so there will be fork for each client request as usual. What I meant was no fork/thread to handle timeout separately.

So your poll() will wait on accept() or timeout. If accept() comes out (new client request), it forks a process to handle the new client request as in a normal TCP concurrent server. The parent process does the time computation I said and comes back to call poll again to wait on remaining timeout and accept. The small time delay to fork() etc. does not matter. If it comes out on a timeout, there is no fork(), it just connects to servers to get loads and then comes back to wait on poll() again with a fresh 5 second timeout and accept() for client requests.

Since S1 and S2 are not run at fixed ports but take the port no.s as command line arguments, Load Balancer should also take the port no.s of both S1 and S2 (along with its own) so that it can connect to S1 and S2. Sl L will take 3 arguments in the this order (order is important for TAs to check):
<own port> <S1's port> <S2's port>

For IP, assume they are all localhost, so 127.0.0.1 only for the assignment.

And the client will also need to take as command line argument the port of L in the order <client port> <L's port> :-( I am getting rusty...

Sorry. But this is a very small change anyway.
